\subsection{Exkurs: Objekterkennung}

Für die Identifizierung des gesuchten Plüschtiers wird eine optische Objekterkennung genutzt.
Das RB-Camera-WW-Modul kann über die \ac{CSI} Schnittstelle mit dem Raspberry Pi verbunden werden und unter Raspbian ohne die Installation von zusätzlichen Softwarepaketen ausgelesen werden.\ifootcite{raspicam}

Unter Objekterkennung versteht man das Verfahren zum Identifizieren bekannter Objekte innerhalb eines Objektraums. So wird z.B. das Vorhandensein eines Objektes in einem digitalen Bild und dessen Position und Lage bestimmt.
Dieses Vorgehen ist nicht zu verwechseln mit der Objektklassifizierung, bei welcher ein Objekt lediglich auf Zugehörigkeit zu zuvor definierten Kategorien untersucht wird.
Derzeit wird grundsätzlich zwischen zwei Methoden der Objekterkennung unterschieden.
Beim Ansatz des maschinellen Lernens werden zunächst manuell Merkmale definiert, welche dann mit Methoden, wie z.B. der Viola-Jones-Methode, maschinell erkannt werden können.\ifootcite{viola_jones}
Der Deep-Learning-Ansatz hingegen setzt lediglich eine große Menge zuvor klassifizierter Objekte voraus.
Unter Verwendung eines \ac{CNN} setzen Girshick, Donahue, Darrell und Malik 2013 mit \ac{R-CNN} diesen Ansatz erstmals um.\ifootcite{rcnn}
Mit dem \ac{SSD} Ansatz und dem \ac{YOLO} Ansatz wurden in 2015 die Anforderungen an Rechenkapazität für die Durchführung der Objekterkennung deutlich reduziert.\ifootcite{ssd,yolov1}
In 2018 veröffentlichten Redmon und Farhadi die dritte Version des \ac{YOLO} Algorithmus, welche z.Z. die schnellste und gleichzeit auch eine akkurate Objekterkennung ist.\ifootcite{yolov3}
\ac{YOLO} in der Version 3 wendet im Gegensatz zu vorherigen Methoden ein einziges neuronales Netz auf das gesamte Bild an. Dieses Netzwerk unterteilt das Bild in Regionen und sagt für jede Region Bounding Boxes und Wahrscheinlichkeiten voraus.
So betrachtet der Algorithmus das gesamte Bild zur Testzeit, wodurch auch der globale Kontext berücksichtigt wird. Die Vorhersagen werden mit einer einzigen Netzwerkauswertung erstellt, im Gegensatz zu Systemen wie \ac{R-CNN}, die Tausende für ein einziges Bild benötigen, da die Erkennung von potentiellen Objekten nicht vom selben neuralen Netz ausgeführt wird.\ifootcite{yolov3_homepage}

So setzt der \ac{AAR}, der geringen Erkennungszeiten wegen, auf den Algorithmus \ac{YOLO} in der Version 3.

\subsubsection{Training eines benutzerdefinierten YOLOv3 Modells}

Für die Erkennung der Plüschtiere mit \ac{YOLO}v3 muss zunächst ein bestehendes \ac{YOLO}-Modell auf die neuen Objektklassen eingelernt werden.
Dazu werden von jeder Objektklasse idealerweise tausende Bilder benötigt, welche in unterschiedlichen Lichtverhältnissen, Hintergründen, Winkeln, Kombinationen und Entfernungen aufgenommen wurden.
Diese Bilder müssen dann anschließend manuell klassifiziert werden.

\dhbwFigure{%
    caption	= Aufnehmen von Trainingsdaten,
    label	= fig:capture_training_data,
    path	= content/assets/training_data_capture_setup.jpg,
    source	= Eigene Darstellung
}

\autoref{fig:capture_training_data} zeigt die Aufzeichnung unserer Trainingsdaten.
Insgesamt wurden über 13000 Bilder (mehr als 2000 pro Objektklasse) erstellt und einzeln manuell mit Hilfe des Tools LabelImg klassifiziert.

\dhbwFigure{%
    caption	= Labeln von Trainingsdaten,
    label	= fig:labelimg,
    path	= content/assets/labelimg.png,
    source	= Eigene Darstellung,
    width	= 0.5\textwidth
}

Beim Labeln muss um jedes zu klassifizierende Objekt manuell mit einer Bounding Box gekennzeichnet werden.
So muss bspw. der Tiger in \autoref{fig:labelimg} mit einer Bounding Box eingeschlossen und mit dem Label \enquote{tiger} versehen werden.

Neben den Trainingsdaten muss auch ein Basismodell ausgewählt und eine Konfiguration für das Training des neuen Modells erstellt werden.
Es gibt verschiedene \ac{YOLO}-Modelle.
Mit dem Modell \enquote{yolov3-tiny} können bspw. mit einer Pascal Titan X bis zu 220 Bilder pro Sekunde verarbeitet werden.\ifootcite{yolov3_homepage}

\begin{dhbwtable}{%
    caption	= Performance verschiedener Modelle,
    label	= tab:model_perf,
    source	= Eigene Messungen,
}
    \begin{tabular}{llll}
        \toprule
        \textbf{Modell}     & \textbf{Auflösung}    & \textbf{Bilder pro Sekunde}   & \textbf{Genauigkeit}\\\midrule
        yolov3-tiny         & 128x128               & 6                             & 4\\
        yolov3-tiny         & 192x128               & 5                             & 6\\
        yolov3-tiny         & 256x192               & 3                             & 7\\
        yolov3-tiny-3l      & 192x128               & 4                             & 8\\
        yolov3-tiny-3l      & 256x192               & 2                             & 9\\
        yolov3              & 192x128               & 0                             & 0\\
        yolov3              & 128x128               & 0                             & 0\\\bottomrule
    \end{tabular}    
\end{dhbwtable}

\autoref{tab:model_perf} zeigt die Performance verschieden konfigurierter Modelle zur Erkennung der Plüschtiere auf dem Raspberry Pi.
Jedes Modell wurde über 10000 Iterationen mit Hilfe der Bibliothek \enquote{darknet} von GitHub Nutzer \enquote{AlexeyB} auf einer kostenlosen virtuellen Maschine von Google Colab eingelernt.
Die Genauigkeit wurde auf einer Skala von 0-10 definiert, wobei 10 bedeuten würde, dass alle getesteten Fälle so erkannt wurden, dass der \ac{AAR} seine Aufgabe erfüllen könnte.
Da die Objekterkennung nicht mehr als zwei mal pro Sekunde durchgeführt werden muss nutzt der \ac{AAR} die Modellkonfiguration \enquote{yolov3-tiny-3l-256x192}, wobei \enquote{3l} für die dritte \ac{YOLO}-Schicht im Netzwerk steht.

\autoref{app:object_detection_training_ad} fasst die Aktivitäten beim 
Erstellen eines benutzerdefiniterten \ac{YOLO}v3 Modells in Form eines Aktivitätendiagramms zusammen.

\subsubsection{Ablauf der Objekterkennung auf dem Raspberry Pi}

Für die eigentliche Objekterkennung wird auf dem Raspberry Pi zuerst ein separater Thread gestartet.
Darin wird zunächst das benutzerdefinierte \ac{YOLO}-Modell in den Zwischenspeicher geladen.
Anschließend wird mit der Bibliothek OpenCV ein Bild, in der Auflösung des verarbeitenden \ac{YOLO}-Modells, über die \ac{CSI}-Schnittstelle aufgenommen.
Das Bild wird dann mit OpenCV und dem \ac{YOLO}-Modell ausgewertet.
Anschließend wird überprüft ob die Temperatur der \ac{CPU} noch im zulässigen Bereich ist.
Sollte die \ac{CPU} zu warm geworden sein wird gewartet, bis die Temperatur abgenommen hat.
Das Aufnehmen eines Bildes, dessen Verarbeitung und die Überwachung der \ac{CPU}-Temperatur wird wiederholt, bis der Thread extern gestoppt wird.
\autoref{app:actual_objecti_detection_ad} fasst die Aktivitäten bei der Objekterkennung auf dem Raspberry Pi in Form eines Aktivitätendiagramms zusammen.